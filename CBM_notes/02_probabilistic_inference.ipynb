{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7226b78b",
   "metadata": {},
   "source": [
    "\n",
    "# Concept Bottleneck Models â€” Probabilistic Inference View\n",
    "\n",
    "## 1. From Causal Structure to Inference\n",
    "\n",
    "In the causal view, we assumed the structure:\n",
    "\n",
    "$$\n",
    "X \\rightarrow C \\rightarrow Y\n",
    "$$\n",
    "\n",
    "This notebook focuses on **how inference is performed** in Concept Bottleneck Models (CBMs), and what approximations are made in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7680108",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Exact Probabilistic Inference\n",
    "\n",
    "Given the CBM factorization:\n",
    "\n",
    "$$\n",
    "p(Y \\mid X) = \\sum_C p(Y \\mid C) p(C \\mid X)\n",
    "$$\n",
    "\n",
    "This equation requires summing over **all possible concept configurations**.\n",
    "\n",
    "If concepts are binary and there are $K$ concepts:\n",
    "\n",
    "$$\n",
    "|\\mathcal{C}| = 2^K\n",
    "$$\n",
    "\n",
    "Exact inference quickly becomes intractable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9eceef",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Concept Posterior Distribution\n",
    "\n",
    "The concept encoder models the posterior:\n",
    "\n",
    "$$\n",
    "p(C \\mid X)\n",
    "$$\n",
    "\n",
    "In practice, this is approximated by a neural network:\n",
    "\n",
    "$$\n",
    "\\hat{C} = g_\\theta(X)\n",
    "$$\n",
    "\n",
    "This network performs **amortized inference**: instead of solving a new inference problem for each input, it learns a global mapping from $X$ to $C$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b086400",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Deterministic Approximation\n",
    "\n",
    "Rather than marginalizing over all $C$, CBMs use a point estimate:\n",
    "\n",
    "$$\n",
    "C \\approx \\hat{C}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "p(Y \\mid X) \\approx p(Y \\mid C = \\hat{C})\n",
    "$$\n",
    "\n",
    "This leads to the standard CBM prediction:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = f_\\phi(g_\\theta(X))\n",
    "$$\n",
    "\n",
    "This approximation trades exact probabilistic reasoning for computational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d17ce",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Stochastic vs Deterministic Concepts\n",
    "\n",
    "Two inference strategies exist:\n",
    "\n",
    "### Deterministic CBMs\n",
    "- Use $\\hat{C} = \\mathbb{E}[C \\mid X]$\n",
    "- Simple and efficient\n",
    "- Most common in practice\n",
    "\n",
    "### Stochastic CBMs\n",
    "- Sample $C \\sim p(C \\mid X)$\n",
    "- Approximate the sum via Monte Carlo\n",
    "- More faithful to probabilistic modeling but harder to train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2c72e",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Relation to Variational Inference\n",
    "\n",
    "CBMs resemble variational models:\n",
    "\n",
    "- Concepts act as latent variables\n",
    "- The encoder approximates $p(C \\mid X)$\n",
    "- The predictor approximates $p(Y \\mid C)$\n",
    "\n",
    "However, unlike VAEs:\n",
    "- Concepts are **observed and supervised**\n",
    "- There is no KL regularization term\n",
    "- Latent variables are semantically meaningful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7a6d1",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Bias Introduced by Point Estimates\n",
    "\n",
    "Using $\\hat{C}$ instead of full marginalization introduces bias:\n",
    "\n",
    "$$\n",
    "p(Y \\mid X) \\neq p(Y \\mid \\hat{C})\n",
    "$$\n",
    "\n",
    "This bias increases when:\n",
    "- Concept uncertainty is high\n",
    "- Concepts are poorly predicted\n",
    "- The decision boundary is nonlinear\n",
    "\n",
    "This explains why CBMs are sensitive to concept quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad4112",
   "metadata": {},
   "source": [
    "\n",
    "## 8. When Exact Inference Matters\n",
    "\n",
    "Exact or stochastic inference is important when:\n",
    "- Concepts are uncertain\n",
    "- Downstream decisions are sensitive to small changes\n",
    "- Safety-critical applications require calibrated uncertainty\n",
    "\n",
    "In such cases, stochastic CBMs or Bayesian variants are preferable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08ca85",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "- CBMs rely on a probabilistic factorization\n",
    "- Exact inference is intractable for many concepts\n",
    "- Practical CBMs use deterministic amortized inference\n",
    "- This introduces bias but improves scalability\n",
    "- Concept uncertainty plays a central role in performance\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}