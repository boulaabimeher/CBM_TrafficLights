{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab39d45",
   "metadata": {},
   "source": [
    "# 03 — Loss Functions and Gradient Flow in Concept Bottleneck Models\n",
    "\n",
    "This notebook provides a **deep, clear, and mathematically grounded** explanation of how loss functions are defined in CBMs and how gradients propagate through the bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128d864",
   "metadata": {},
   "source": [
    "## 1. CBM Recap (Functional Form)\n",
    "\n",
    "$$X \\xrightarrow{g_\\theta} \\hat{C} \\xrightarrow{f_\\phi} \\hat{Y}$$\n",
    "\n",
    "- $g_\\theta$: concept predictor\n",
    "- $f_\\phi$: label predictor\n",
    "- $C$: ground-truth concepts\n",
    "- $Y$: ground-truth task label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856b0a0",
   "metadata": {},
   "source": [
    "## 2. Loss Decomposition\n",
    "\n",
    "CBMs usually optimize a **multi-objective loss**:\n",
    "\n",
    "$$\\mathcal{L} = \\lambda_c\\,\\mathcal{L}_{concept} + \\lambda_y\\,\\mathcal{L}_{task}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}_{concept} = \\ell(\\hat{C}, C)$\n",
    "- $\\mathcal{L}_{task} = \\ell(\\hat{Y}, Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf0235",
   "metadata": {},
   "source": [
    "## 3. Gradient Flow (Critical Insight)\n",
    "\n",
    "The total gradient wrt concept parameters $\\theta$ is:\n",
    "\n",
    "$$\\nabla_\\theta \\mathcal{L} = \\lambda_c\\nabla_\\theta \\mathcal{L}_{concept} + \\lambda_y\\nabla_{\\hat{C}} \\mathcal{L}_{task} \\cdot \\nabla_\\theta \\hat{C}$$\n",
    "\n",
    "This explains **why CBMs are unstable** if losses are unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9610381e",
   "metadata": {},
   "source": [
    "## 4. Why Sequential Training Works\n",
    "\n",
    "### Step 1: Train concepts\n",
    "$$\\min_\\theta \\mathcal{L}_{concept}$$\n",
    "\n",
    "### Step 2: Freeze $g_\\theta$, train classifier\n",
    "$$\\min_\\phi \\mathcal{L}_{task}(f_\\phi(C))$$\n",
    "\n",
    "✔ Concept purity\n",
    "✘ No task feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43bd03",
   "metadata": {},
   "source": [
    "## 5. Why Joint Training Helps (and Hurts)\n",
    "\n",
    "Joint training allows task gradients to refine concepts, but:\n",
    "\n",
    "- Concepts may drift\n",
    "- Semantic meaning may degrade\n",
    "\n",
    "This is the **core CBM trade-off**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813eda6",
   "metadata": {},
   "source": [
    "## 6. Common Failure Modes\n",
    "\n",
    "- Concept leakage\n",
    "- Shortcut learning\n",
    "- Concept-task gradient conflict\n",
    "- Over-regularized bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5af38",
   "metadata": {},
   "source": [
    "## 7. Research Insight\n",
    "\n",
    "CBMs are best understood as **regularized latent-variable models with semantic constraints**.\n",
    "\n",
    "The bottleneck is not just architectural — it is **optimization-sensitive**."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}