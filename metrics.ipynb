{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acdcbb9e",
   "metadata": {},
   "source": [
    "## Step #1: Train a CBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8101c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user_cril/meher/CRIL/CBM/TrafficLights/traffic-env/lib/python3.10/site-packages/torch_concepts/nn/__init__.py:18: FutureWarning: The 'torch_concepts.nn.mid' module contains experimental APIs that are unstable and subject to change without notice. If you are using these classes intentionally, be aware that breaking changes may occur in future releases. Consider using the high-level API (torch_concepts.nn.high) for stable interfaces.\n",
      "  from .modules.mid.base.model import BaseConstructor\n",
      "INFO:root:We found a dataset previously generated with the same config that has been cached.\n",
      "INFO:root:\tIf you wish to re-generate it, please use regenerate=True.\n",
      "INFO:root:We found a dataset previously generated with the same config that has been cached.\n",
      "INFO:root:\tIf you wish to re-generate it, please use regenerate=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 300 samples while test set has 100 samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_concepts.nn import LinearZC\n",
    "from torch_concepts.data.datasets.traffic import TrafficLights\n",
    "\n",
    "# Fix seeds first\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "n_samples = 500\n",
    "\n",
    "# Loading training dataset\n",
    "dataset = TrafficLights(\n",
    "    n_samples=n_samples,\n",
    "    possible_starting_directions=['west'],\n",
    "    resize_final_image=0.05,\n",
    "    selected_concepts=[\n",
    "        'green light on selected lane',\n",
    "        'car in intersection',\n",
    "        'ambulance seen',\n",
    "        'ambulance approaching perpendicular to selected car',\n",
    "    ],\n",
    "    split='train',\n",
    ")\n",
    "concept_names, task_names = dataset.concept_names, dataset.task_names\n",
    "n_concepts = len(concept_names)\n",
    "\n",
    "# Loading testing dataset\n",
    "# Generate the test dataset\n",
    "test_dataset = TrafficLights(\n",
    "    n_samples=n_samples,\n",
    "    possible_starting_directions=['west'],\n",
    "    resize_final_image=0.05,\n",
    "    selected_concepts=[\n",
    "        'green light on selected lane',\n",
    "        'car in intersection',\n",
    "        'ambulance seen',\n",
    "        'ambulance approaching perpendicular to selected car',\n",
    "    ],\n",
    "    split='test',\n",
    ")\n",
    "print(\n",
    "    f\"Training set has {len(dataset)} samples while test set \"\n",
    "    f\"has {len(test_dataset)} samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a320fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (6): LeakyReLU(negative_slope=0.01)\n",
      "    (7): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (8): LeakyReLU(negative_slope=0.01)\n",
      "    (9): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): MaxPool2d(kernel_size=(5, 5), stride=(5, 5), padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Flatten(start_dim=1, end_dim=-1)\n",
      "    (12): Linear(in_features=576, out_features=32, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (1): LinearZC(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=4, bias=True)\n",
      "      (1): Unflatten(dim=-1, unflattened_size=(4,))\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Dimensions\n",
    "# -----------------------\n",
    "latent_dims = 32\n",
    "n_concepts = len(concept_names)\n",
    "\n",
    "# -----------------------\n",
    "# Encoder: X â†’ Z\n",
    "# -----------------------\n",
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 4, (3, 3), padding='same'),\n",
    "    torch.nn.LeakyReLU(),\n",
    "\n",
    "    torch.nn.Conv2d(4, 4, (3, 3), padding='same'),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.BatchNorm2d(4),\n",
    "\n",
    "    torch.nn.Conv2d(4, 4, (3, 3), padding='same'),\n",
    "    torch.nn.LeakyReLU(),\n",
    "\n",
    "    torch.nn.Conv2d(4, 4, (3, 3), padding='same'),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.BatchNorm2d(4),\n",
    "\n",
    "    torch.nn.MaxPool2d((5, 5)),\n",
    "\n",
    "    torch.nn.Flatten(start_dim=1),\n",
    "    torch.nn.Linear(576, latent_dims),\n",
    "    torch.nn.LeakyReLU(),\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Concept layer: Z â†’ C\n",
    "# -----------------------\n",
    "c_layer = LinearZC(\n",
    "    in_features=latent_dims,\n",
    "    out_features=n_concepts,\n",
    ")\n",
    "\n",
    "# ðŸ”‘ Attach concept semantics manually\n",
    "c_layer.annotations = list(concept_names)\n",
    "\n",
    "# -----------------------\n",
    "# Task predictor: C â†’ Y\n",
    "# -----------------------\n",
    "y_predictor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_concepts, latent_dims),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(latent_dims, 1),  # binary task\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Full CBM\n",
    "# -----------------------\n",
    "model = torch.nn.Sequential(\n",
    "    encoder,\n",
    "    c_layer,\n",
    "    y_predictor,\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897cd6b",
   "metadata": {},
   "source": [
    "## We train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4b1a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Step [1/6], Loss: 3.0957, Task Accuracy: 96.00%, \n",
      "Epoch [10/20], Step [1/6], Loss: 1.9443, Task Accuracy: 90.00%, \n",
      "Epoch [15/20], Step [1/6], Loss: 1.9977, Task Accuracy: 90.00%, \n",
      "Epoch [20/20], Step [1/6], Loss: 1.1628, Task Accuracy: 92.00%, \n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n_epochs = 20\n",
    "concept_loss_weight = 10\n",
    "lr = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "# Define optimizer and loss function\n",
    "model = torch.nn.Sequential(encoder, c_layer, y_predictor)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Make a batch dataset loader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Standard PyTorch learning cycle\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_idx, (x, y, c, _, _) in enumerate(dataloader):\n",
    "        # Encode input, then predict concept and downstream tasks activations\n",
    "        emb = encoder(x)\n",
    "        c_pred = c_layer(emb).sigmoid()\n",
    "        y_pred = y_predictor(c_pred).sigmoid().view(-1)\n",
    "\n",
    "        # Double loss on concepts and tasks\n",
    "        loss = loss_fn(y_pred, y) + concept_loss_weight * loss_fn(c_pred, c)\n",
    "\n",
    "        # Perform the update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        task_acc = torch.mean(((y_pred > 0.5) == y).type(torch.float))\n",
    "        task_acc = task_acc.detach().cpu().numpy()\n",
    "        if ((epoch + 1) % 5 == 0) and (batch_idx == 0):\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{n_epochs}], \"\n",
    "                f\"Step [{batch_idx+1}/{len(dataloader)}], \"\n",
    "                f\"Loss: {loss.item():.4f}, \"\n",
    "                f\"Task Accuracy: {task_acc * 100:.2f}%, \"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3caccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train set to memory\n",
    "x_train = []\n",
    "c_train = []\n",
    "y_train = []\n",
    "for (x, y, c, _, _) in dataset:\n",
    "    x_train.append(x.unsqueeze(0))\n",
    "    y_train.append(y.unsqueeze(0))\n",
    "    c_train.append(c.unsqueeze(0))\n",
    "x_train = torch.concat(x_train, dim=0)\n",
    "y_train = torch.concat(y_train, dim=0)\n",
    "c_train = torch.concat(c_train, dim=0)\n",
    "\n",
    "# Load the test set to memory\n",
    "x_test = []\n",
    "c_test = []\n",
    "y_test = []\n",
    "for (x, y, c, _, _) in test_dataset:\n",
    "    x_test.append(x.unsqueeze(0))\n",
    "    y_test.append(y.unsqueeze(0))\n",
    "    c_test.append(c.unsqueeze(0))\n",
    "x_test = torch.concat(x_test, dim=0)\n",
    "y_test = torch.concat(y_test, dim=0)\n",
    "c_test = torch.concat(c_test, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e758a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average task prediction: [0.5466324]\n",
      "Average concept prediction: [0.6060981  0.28488654 0.10826973 0.0669183 ]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "c_pred = c_layer(encoder(x_test)).sigmoid()\n",
    "y_pred = y_predictor(c_pred).sigmoid()\n",
    "print(\"Average task prediction:\", y_pred.mean(0).detach().cpu().numpy())\n",
    "print(\"Average concept prediction:\", c_pred.mean(0).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854c176",
   "metadata": {},
   "source": [
    "## Step #2: Compute task and concept performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7da0718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task performance: 94.58%\n",
      "Concept performance: 92.12%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "concept_performance = roc_auc_score(c_test, c_pred.detach())\n",
    "task_performance = roc_auc_score(y_test, y_pred.detach())\n",
    "\n",
    "print(f'Task performance: {task_performance*100:.2f}%')\n",
    "print(f'Concept performance: {concept_performance*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6a890",
   "metadata": {},
   "source": [
    "## Step #3: Compute intervention effectiveness\n",
    "\n",
    "    Cannot find the intervention_score : \n",
    "\n",
    "    so we created a custom function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e73bbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'intervention_score' from 'torch_concepts.nn' (/home/user_cril/meher/CRIL/CBM/TrafficLights/traffic-env/lib/python3.10/site-packages/torch_concepts/nn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_concepts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m intervention_score\n\u001b[1;32m      2\u001b[0m intervention_groups \u001b[38;5;241m=\u001b[39m [[], [\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate intervention effectiveness of each concept group individually\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'intervention_score' from 'torch_concepts.nn' (/home/user_cril/meher/CRIL/CBM/TrafficLights/traffic-env/lib/python3.10/site-packages/torch_concepts/nn/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torch_concepts.nn import intervention_score\n",
    "intervention_groups = [[], [0], [1], [0, 1]]\n",
    "\n",
    "# Evaluate intervention effectiveness of each concept group individually\n",
    "intervention_scores = intervention_score(\n",
    "    y_predictor,\n",
    "    c_pred,\n",
    "    c_test,\n",
    "    y_test,\n",
    "    intervention_groups,\n",
    "    auc=False,\n",
    ")\n",
    "print(f'Individual intervention scores: {intervention_scores}')\n",
    "\n",
    "# Evaluate the global intervention effectiveness as the AUC\n",
    "intervention_auc = intervention_score(\n",
    "    y_predictor,\n",
    "    c_pred,\n",
    "    c_test,\n",
    "    y_test,\n",
    "    intervention_groups,\n",
    ")\n",
    "print(f'Intervention AUC: {intervention_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "810c6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def intervention_score_cbm(\n",
    "    predictor,\n",
    "    c_pred,\n",
    "    c_true,\n",
    "    y_true,\n",
    "    intervention_groups,\n",
    "    auc=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Correct reimplementation of intervention score from the CBM literature.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true_np = y_true.view(-1).cpu().numpy()\n",
    "    scores = []\n",
    "    num_intervened = []\n",
    "\n",
    "    for group in intervention_groups:\n",
    "        c_intervened = c_pred.clone()\n",
    "\n",
    "        if len(group) > 0:\n",
    "            c_intervened[:, group] = c_true[:, group]\n",
    "\n",
    "        y_logits = predictor(c_intervened).view(-1)\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().numpy()\n",
    "\n",
    "        score = roc_auc_score(y_true_np, y_probs)\n",
    "\n",
    "        scores.append(score)\n",
    "        num_intervened.append(len(group))\n",
    "\n",
    "    if not auc:\n",
    "        return scores\n",
    "\n",
    "    # ðŸ”‘ Sort by number of intervened concepts\n",
    "    num_intervened = np.array(num_intervened)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    order = np.argsort(num_intervened)\n",
    "    num_intervened = num_intervened[order]\n",
    "    scores = scores[order]\n",
    "\n",
    "    # ðŸ”‘ Normalize x-axis to [0, 1]\n",
    "    x = num_intervened / num_intervened.max()\n",
    "\n",
    "    # ðŸ”‘ Area under intervention curve\n",
    "    intervention_auc = np.trapz(scores, x)\n",
    "\n",
    "    return intervention_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2468fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual intervention scores: [0.9458333333333333, 0.9420833333333334, 0.9691666666666667, 0.9570833333333333]\n",
      "Intervention AUC: 0.9535\n"
     ]
    }
   ],
   "source": [
    "intervention_groups = [\n",
    "    [],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0, 1],\n",
    "]\n",
    "\n",
    "scores = intervention_score_cbm(\n",
    "    predictor=y_predictor,\n",
    "    c_pred=c_pred,\n",
    "    c_true=c_test,\n",
    "    y_true=y_test,\n",
    "    intervention_groups=intervention_groups,\n",
    "    auc=False,\n",
    ")\n",
    "\n",
    "print(\"Individual intervention scores:\", scores)\n",
    "\n",
    "auc = intervention_score_cbm(\n",
    "    predictor=y_predictor,\n",
    "    c_pred=c_pred,\n",
    "    c_true=c_test,\n",
    "    y_true=y_test,\n",
    "    intervention_groups=intervention_groups,\n",
    "    auc=True,\n",
    ")\n",
    "\n",
    "print(f\"Intervention AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee56f0",
   "metadata": {},
   "source": [
    "## Step #4: Compute concept completeness \n",
    "To compute concept completeness, we need a black-box baseline model that uses both raw features and concept labels, as this matches the information provided to the CBM. The following code implements a simple black-box model with a similar parameter count to the CBM for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef92cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maps containing the values of each concept\n",
    "c_train_maps = c_train.unsqueeze(-1).unsqueeze(-1)\n",
    "c_train_maps = c_train_maps.expand(-1, -1, 64, 64)\n",
    "c_test_maps = c_test.unsqueeze(-1).unsqueeze(-1)\n",
    "c_test_maps = c_test_maps.expand(-1, -1, 64, 64)\n",
    "\n",
    "# Put them together with the input features\n",
    "xc_train = torch.concat((x_train, c_train_maps), dim=1)\n",
    "xc_test = torch.concat((x_test, c_test_maps), dim=1)\n",
    "\n",
    "# Defining a balck box baseline\n",
    "baseline = torch.nn.Sequential(\n",
    "    # A 3x3 convolution with 4 output channels\n",
    "    torch.nn.Conv2d(3 + n_concepts, 4, (3, 3), padding='same'),\n",
    "    torch.nn.LeakyReLU(),\n",
    "\n",
    "    # A 3x3 convolution with 4 output channels with a batch norm\n",
    "    torch.nn.Conv2d(4, 4, (3, 3), padding='same'),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.BatchNorm2d(4),\n",
    "\n",
    "    # A 3x3 convolution with 4 output channels\n",
    "    torch.nn.Conv2d(4, 4, (3, 3), padding='same'),\n",
    "    torch.nn.LeakyReLU(),\n",
    "\n",
    "    # A 3x3 convolution with 4 output channels with a batch norm\n",
    "    torch.nn.Conv2d(4, 4, (3, 3), padding='same'),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.BatchNorm2d(4),\n",
    "\n",
    "    # A 5x5 max pooling layer\n",
    "    torch.nn.MaxPool2d((5, 5)),\n",
    "\n",
    "    # Finally, we flatten and map it to a known latent space size\n",
    "    torch.nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    torch.nn.Linear(576, 1),\n",
    ")\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(baseline.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Standard PyTorch learning cycle\n",
    "baseline.train()\n",
    "for epoch in range(n_epochs):\n",
    "   optimizer.zero_grad()\n",
    "\n",
    "   # Encode input, then predict concept and downstream tasks activations\n",
    "   y_pred_baseline = baseline(xc_train).sigmoid().view(-1)\n",
    "\n",
    "   # Double loss on concepts and tasks\n",
    "   loss = loss_fn(y_pred_baseline, y_train)\n",
    "   loss.backward()\n",
    "   optimizer.step()\n",
    "\n",
    "baseline.eval()\n",
    "y_pred_baseline = baseline(xc_test).sigmoid()\n",
    "task_performance_baseline = roc_auc_score(y_test, y_pred_baseline.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56fdb886",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_concepts.metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_concepts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m completeness_score\n\u001b[1;32m      3\u001b[0m concept_completeness \u001b[38;5;241m=\u001b[39m completeness_score(y_test, y_pred_baseline, y_pred)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask performance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_performance\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_concepts.metrics'"
     ]
    }
   ],
   "source": [
    "from torch_concepts.metrics import completeness_score\n",
    "\n",
    "concept_completeness = completeness_score(y_test, y_pred_baseline, y_pred)\n",
    "\n",
    "print(f'Task performance: {task_performance*100:.2f}%')\n",
    "print(f'Task performance baseline: {task_performance_baseline*100:.2f}%')\n",
    "print(f'Concept completeness: {concept_completeness*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac96a1",
   "metadata": {},
   "source": [
    "## Create a custom fct for completness score \n",
    "    can u verify it (Tanmoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40414884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_score_cbm(\n",
    "    y_true,\n",
    "    y_pred_baseline,\n",
    "    y_pred_cbm,\n",
    "    metric=\"auc\",\n",
    "):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "    # -----------------------\n",
    "    # Detach tensors safely\n",
    "    # -----------------------\n",
    "    def to_numpy(x):\n",
    "        if torch.is_tensor(x):\n",
    "            return x.detach().cpu().numpy().ravel()\n",
    "        return np.asarray(x).ravel()\n",
    "\n",
    "    y_true = to_numpy(y_true)\n",
    "    y_pred_baseline = to_numpy(y_pred_baseline)\n",
    "    y_pred_cbm = to_numpy(y_pred_cbm)\n",
    "\n",
    "    # -----------------------\n",
    "    # Compute performance\n",
    "    # -----------------------\n",
    "    if metric == \"auc\":\n",
    "        perf_baseline = roc_auc_score(y_true, y_pred_baseline)\n",
    "        perf_cbm = roc_auc_score(y_true, y_pred_cbm)\n",
    "    elif metric == \"accuracy\":\n",
    "        perf_baseline = accuracy_score(y_true, y_pred_baseline > 0.5)\n",
    "        perf_cbm = accuracy_score(y_true, y_pred_cbm > 0.5)\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'auc' or 'accuracy'\")\n",
    "\n",
    "    if perf_baseline >= 1.0:\n",
    "        return 0.0\n",
    "\n",
    "    completeness = (perf_cbm - perf_baseline) / (1.0 - perf_baseline)\n",
    "    return max(0.0, completeness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae8467ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task performance (CBM): 94.58%\n",
      "Task performance (baseline): 97.46%\n",
      "Concept completeness: 0.00%\n"
     ]
    }
   ],
   "source": [
    "concept_completeness = completeness_score_cbm(\n",
    "    y_true=y_test,\n",
    "    y_pred_baseline=y_pred_baseline,\n",
    "    y_pred_cbm=y_pred,\n",
    "    metric=\"auc\",\n",
    ")\n",
    "\n",
    "print(f\"Task performance (CBM): {task_performance*100:.2f}%\")\n",
    "print(f\"Task performance (baseline): {task_performance_baseline*100:.2f}%\")\n",
    "print(f\"Concept completeness: {concept_completeness*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f6ae5",
   "metadata": {},
   "source": [
    "## Quick diagnostic (recommended)\n",
    "ðŸ”¹ Check unclipped completeness (for insight only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c328b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw completeness: -1.131\n"
     ]
    }
   ],
   "source": [
    "raw_completeness = (\n",
    "    task_performance - task_performance_baseline\n",
    ") / (1 - task_performance_baseline)\n",
    "\n",
    "print(f\"Raw completeness: {raw_completeness:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
